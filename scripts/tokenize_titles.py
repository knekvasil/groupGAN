# scripts/tokenize_titles.py
"""
This script processes the cleaned text titles from a CSV file, trains a
Hugging Face `tokenizers` library tokenizer on the text data, encodes the
text titles into sequences of token IDs, and then splits the data into
training, validation, and test sets. Finally, it saves the trained tokenizer
and the data splits as NumPy files.

It assumes the cleaned data is available in a CSV file with 'title' and 'label'
columns, typically generated by the `clean.py` script.
"""

import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from tokenizers import (
    Tokenizer,
    models,
    normalizers,
    pre_tokenizers,
    trainers,
)

# Paths
# Path to the cleaned input CSV file
IN_PATH = "../project_data/processed_titles.csv"
# Path to save the trained Hugging Face tokenizer (.json file)
TOKENIZER_PATH = (
    "../project_data/hf_tokenizer.json"  # Hugging Face tokenizers save as .json
)
# Directory to save the processed data splits (numpy files)
OUT_DIR = "../project_data/processed/"

# Config
# Vocabulary size for the tokenizer
VOCAB_SIZE = 10000  # Adjusted based on your second script, common size
# Fixed length for the tokenized sequences (padding/truncation)
SEQUENCE_LENGTH = 20  # Adjusted based on your second script, common name


def main():
    """
    Main function to execute the tokenization, encoding, and splitting pipeline.

    Loads cleaned data, configures and trains a Hugging Face WordPiece tokenizer,
    encodes the text titles into padded/truncated token ID sequences, splits
    the sequences and labels into training, validation, and test sets using
    stratified splitting, and saves the tokenizer and the data splits to disk.
    """
    # Ensure output directory exists
    os.makedirs(OUT_DIR, exist_ok=True)
    print(f"Ensured output directory '{OUT_DIR}' exists.")

    # Load data
    try:
        print(f"Loading data from {IN_PATH}...")
        df = pd.read_csv(IN_PATH)
        # Ensure 'title' column is treated as string and handle potential NaNs
        texts = df["title"].astype(str).fillna("").tolist()
        # Ensure 'label' column exists and is in a usable format (e.g., int or float)
        # Adjust 'label' column name if it's different in your CSV
        if "label" not in df.columns:
            print("Error: 'label' column not found in the input CSV.")
            # Handle this case - maybe raise an error or exit
            return
        labels = df["label"].tolist()
        print("Data loaded successfully.")

    except FileNotFoundError:
        print(f"Error: Input file not found at {IN_PATH}")
        return
    except KeyError as e:
        print(f"Error: Missing expected column {e} in the input CSV.")
        return
    except Exception as e:
        print(f"An error occurred loading data: {e}")
        return

    print(f"Loaded {len(texts)} titles and {len(labels)} labels.")

    # --- Configure and Train the Hugging Face Tokenizer ---

    # 1. Create a Tokenizer object with a WordPiece model
    # WordPiece is suitable for subword tokenization, often used in models like BERT.
    tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
    print("Tokenizer initialized with WordPiece model.")

    # 2. Set Normalization (e.g., lowercase)
    # Converts text to lowercase before tokenization.
    tokenizer.normalizer = normalizers.Lowercase()
    print("Lowercase normalizer set.")

    # 3. Set Pre-tokenization (e.g., split by whitespace)
    # Splits the text into initial words or tokens based on whitespace.
    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
    print("Whitespace pre-tokenizer set.")

    # 4. Set Post-processing (optional)
    # Post-processing can add special tokens ([CLS], [SEP]) but we handle padding/truncation
    # during encoding for simplicity here.
    # tokenizer.post_processor = ... # Optional post-processing

    # 5. Create a Trainer for the WordPiece model
    # The trainer is used to learn the vocabulary and tokenization rules
    # from the provided text data.
    trainer = trainers.WordPieceTrainer(
        vocab_size=VOCAB_SIZE,  # Desired size of the vocabulary
        special_tokens=[
            "[PAD]",
            "[UNK]",
        ],  # Tokens to be included in the vocabulary regardless of frequency
        min_frequency=2,  # Optional: minimum frequency for a word/subword to be included
        show_progress=True,  # Display training progress
    )
    print(
        f"WordPiece trainer initialized with vocab_size={VOCAB_SIZE}, special_tokens=['[PAD]', '[UNK]']."
    )

    # 6. Train the tokenizer on your text data
    # The tokenizer learns the subword units from the `texts` list.
    print("Training tokenizer...")
    tokenizer.train_from_iterator(texts, trainer=trainer)
    print("Tokenizer training complete.")
    print(f"Vocabulary size after training: {tokenizer.get_vocab_size()}")

    # --- Encode the texts ---

    # Configure padding and truncation *during encoding*
    # This ensures all sequences have the same length (`SEQUENCE_LENGTH`).
    tokenizer.enable_padding(
        direction="right",  # Pad at the end of the sequence
        length=SEQUENCE_LENGTH,  # Target length
        pad_id=tokenizer.token_to_id("[PAD]"),  # ID to use for padding
        pad_token="[PAD]",  # Token string for padding
    )
    tokenizer.enable_truncation(
        max_length=SEQUENCE_LENGTH
    )  # Truncate sequences longer than SEQUENCE_LENGTH
    print(f"Padding and truncation enabled for sequence_length={SEQUENCE_LENGTH}.")

    print("Encoding texts...")
    # Use encode_batch for efficiency when processing multiple texts.
    encodings = tokenizer.encode_batch(texts)
    print("Encoding complete.")

    # Extract token IDs (sequences) from the Encoding objects
    # Each Encoding object contains the token IDs for one input text.
    # We collect the `ids` from each encoding and form a NumPy array.
    padded_sequences = np.array([enc.ids for enc in encodings])

    print(f"Shape of padded sequences: {padded_sequences.shape}")
    # print example encoding
    # if len(encodings) > 0:
    #     print(f"Example sequence: {padded_sequences[0]}")
    #     print(f"Example text: '{texts[0]}'")
    #     print(f"Example tokens: {[tokenizer.id_to_token(id) for id in padded_sequences[0]]}")

    # --- Save the tokenizer ---
    # Save the trained tokenizer configuration and vocabulary to a JSON file.
    tokenizer.save(TOKENIZER_PATH)
    print(f"Hugging Face Tokenizer saved to {TOKENIZER_PATH}")

    # Convert labels to a NumPy array for splitting
    labels_array = np.array(labels)
    print(f"Shape of labels array: {labels_array.shape}")

    # --- Train/Val/Test split ---
    # Split the data into training (80%), validation (10%), and test (10%) sets.
    # `stratify=labels_array` ensures that the proportion of fake and real labels
    # is approximately the same in each split as in the original dataset.
    print("Splitting data into train, validation, and test sets...")
    X_temp, X_test, y_temp, y_test = train_test_split(
        padded_sequences,
        labels_array,
        test_size=0.1,  # 10% for testing
        random_state=42,  # for reproducibility
        stratify=labels_array,  # Stratify helps maintain label distribution
    )
    # Split the temporary set (90%) into training (80% of original) and validation (10% of original).
    # test_size=(0.1 / 0.9) calculates the proportion of the *remaining* data needed for the 10% validation split.
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp,
        y_temp,
        test_size=(0.1 / 0.9),  # 10% of total is (0.1/0.9) of the remaining 90%
        random_state=42,
        stratify=y_temp,  # Stratify based on labels in the temporary set
    )

    print(f"Train shapes: X={X_train.shape}, y={y_train.shape}")
    print(f"Val shapes:   X={X_val.shape}, y={y_val.shape}")
    print(f"Test shapes:  X={X_test.shape}, y={y_test.shape}")

    # --- Save splits ---
    # Save the split data as NumPy files in the output directory.
    try:
        print(f"Saving processed data splits to {OUT_DIR}...")
        np.save(os.path.join(OUT_DIR, "X_train.npy"), X_train)
        np.save(os.path.join(OUT_DIR, "y_train.npy"), y_train)
        np.save(os.path.join(OUT_DIR, "X_val.npy"), X_val)
        np.save(os.path.join(OUT_DIR, "y_val.npy"), y_val)
        np.save(os.path.join(OUT_DIR, "X_test.npy"), X_test)
        np.save(os.path.join(OUT_DIR, "y_test.npy"), y_test)
        print("Processed data saved successfully.")

    except Exception as e:
        print(f"An error occurred saving data: {e}")


if __name__ == "__main__":
    main()
