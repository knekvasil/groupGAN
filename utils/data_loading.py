# utils/data_loading.py
"""
This module provides functionality for loading processed data (numpy arrays
containing token IDs and labels) from files and creating PyTorch DataLoaders
for training, validation, and testing.

It assumes the existence of pre-processed data files in a specified directory,
typically generated by a previous data processing or tokenization step.
"""

import os
import numpy as np
import torch
from torch.utils.data import DataLoader, TensorDataset

# --- Config (Sloppy hardcoded) ---
# TODO: consider making this flexible
PROCESSED_DATA_DIR = "../project_data/processed/"


def get_dataloaders(batch_size: int):
    """
    Loads processed data from .npy files and creates PyTorch DataLoaders
    for train, validation, and test sets.

    The function expects to find the following files in the directory
    specified by PROCESSED_DATA_DIR:
    - X_train.npy: Training features (numpy array of token IDs, shape (N_train, sequence_length))
    - y_train.npy: Training labels (numpy array of integers (0 or 1), shape (N_train,))
    - X_val.npy: Validation features (numpy array of token IDs, shape (N_val, sequence_length))
    - y_val.npy: Validation labels (numpy array of integers (0 or 1), shape (N_val,))
    - X_test.npy: Test features (numpy array of token IDs, shape (N_test, sequence_length))
    - y_test.npy: Test labels (numpy array of integers (0 or 1), shape (N_test,))

    Args:
        batch_size: The batch size to use for the DataLoaders.

    Returns:
        A tuple containing three PyTorch DataLoader instances:
        (train_loader, val_loader, test_loader).
        Returns None or raises FileNotFoundError if data files are not found.

    Raises:
        FileNotFoundError: If any of the required .npy data files are not found
                         in the specified PROCESSED_DATA_DIR.
    """
    # Define file paths (relative to where the script is run, or use absolute paths)
    # Using os.path.join is robust
    X_train_path = os.path.join(PROCESSED_DATA_DIR, "X_train.npy")
    y_train_path = os.path.join(PROCESSED_DATA_DIR, "y_train.npy")
    X_val_path = os.path.join(PROCESSED_DATA_DIR, "X_val.npy")
    y_val_path = os.path.join(PROCESSED_DATA_DIR, "y_val.npy")
    X_test_path = os.path.join(PROCESSED_DATA_DIR, "X_test.npy")
    y_test_path = os.path.join(PROCESSED_DATA_DIR, "y_test.npy")

    # Load data using numpy
    try:
        print(f"Attempting to load data from {PROCESSED_DATA_DIR}...")
        X_train = np.load(X_train_path)
        y_train = np.load(y_train_path)
        X_val = np.load(X_val_path)
        y_val = np.load(y_val_path)
        X_test = np.load(X_test_path)
        y_test = np.load(y_test_path)
        print("Data files loaded successfully.")
    except FileNotFoundError as e:
        print(f"Error loading data files: {e}")
        print(
            "Please ensure the tokenization script has been run and produced the .npy files in the specified directory."
        )
        # Re-raise the exception so the calling script knows loading failed
        raise e

    # Convert to torch tensors
    # X tensors are token IDs, need to be long
    # y tensors are labels (0 or 1), need to be float32 for BCELoss and shape (Batch, 1)
    X_train = torch.tensor(X_train, dtype=torch.long)
    y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(
        1
    )  # Add a dimension for BCELoss
    X_val = torch.tensor(X_val, dtype=torch.long)
    y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(
        1
    )  # Add a dimension for BCELoss
    X_test = torch.tensor(X_test, dtype=torch.long)
    y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(
        1
    )  # Add a dimension for BCELoss

    # Create TensorDatasets
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    test_dataset = TensorDataset(X_test, y_test)

    # Create DataLoaders
    # Shuffle only training data
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False
    )  # No shuffle for test

    print("DataLoaders created successfully.")
    print(f"Train dataset size: {len(train_dataset)}")
    print(f"Val dataset size: {len(val_dataset)}")
    print(f"Test dataset size: {len(test_dataset)}")

    return train_loader, val_loader, test_loader
